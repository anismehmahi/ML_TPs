{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2CSSID-TP04. Arbres de décision et Forêts aléatoires\n",
    "\n",
    "Dans ce TP, nous allons traiter les arbres de décision ainsi que les forêts aléatoires.\n",
    "Dans ce TP, nous allons implémenter ID3 pour les caracéristiques nominales et CART (classement) pour les caracéristiques numériques seulement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binômes : \n",
    "- **Binôme 1 :** MAHMAHI Anis\n",
    "- **Binôme 2 :** BEKKAR Ilhem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.21.5', '1.4.2', '3.5.1')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import numpy             as np\n",
    "import pandas            as pd \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing          import Tuple, List, Type, Union\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**INTRODUCTION**\n",
    "\n",
    "Il existe plusieurs implémentations des arbres de décision :\n",
    "- ID3 (Iterative Dichotomiser 3): dévelopé en 1986 par Ross Quinlan. Il peut être appliqué seulement sur les caractéristiques nominales. Il est utilisé pour le classement.\n",
    "- C4.5: une extension de ID3 par Ross Quinlan. Il peut être appliqué sur tous les types de caractéristiques. Il est utilisé pour le classement.\n",
    "- C5.0: une extension commerciale de C4.5, toujours par Ross Quinlan.\n",
    "- CART (Classification and Regression Trees): comme C4.5 mais utilise d'autres métriques. Aussi, l'algorithme supporte la régression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## I. Réalisation des algorithmes\n",
    "\n",
    "Cette partie sert à améliorer la compréhension des algorithmes d'apprentissage automatique vus en cours en les implémentant à partir de zéro. \n",
    "Pour ce faire, nous allons utiliser la bibliothèque **numpy** qui est utile dans les calcules surtout matricielles.\n",
    "\n",
    "### I.1. ID3\n",
    "\n",
    "Ici, nous allons implémenter l'algorithme vu dans le cours. \n",
    "Nous allons utiliser le dataset \"jouer (nominales)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Le dataset \"jouer\". \n",
    "\n",
    "# temps, temperature, humidite, vent\n",
    "X_jouer = np.array([\n",
    "    ['ensoleile', 'chaude' , 'haute'  , 'non'],\n",
    "    ['ensoleile', 'chaude' , 'haute'  , 'oui'],\n",
    "    ['nuageux'  , 'chaude' , 'haute'  , 'non'],\n",
    "    ['pluvieux' , 'douce'  , 'haute'  , 'non'],\n",
    "    ['pluvieux' , 'fraiche', 'normale', 'non'],\n",
    "    ['pluvieux' , 'fraiche', 'normale', 'oui'],\n",
    "    ['nuageux'  , 'fraiche', 'normale', 'oui'],\n",
    "    ['ensoleile', 'douce'  , 'haute'  , 'non'],\n",
    "    ['ensoleile', 'fraiche', 'normale', 'non'],\n",
    "    ['pluvieux' , 'douce'  , 'normale', 'non'],\n",
    "    ['ensoleile', 'douce'  , 'normale', 'oui'],\n",
    "    ['nuageux'  , 'douce'  , 'haute'  , 'oui'],\n",
    "    ['nuageux'  , 'chaude' , 'normale', 'non'],\n",
    "    ['pluvieux' , 'douce'  , 'haute'  , 'oui']\n",
    "])\n",
    "\n",
    "Y_jouer = np.array(['non', 'non', 'oui', 'oui', 'oui', 'non', 'oui', \n",
    "                    'non', 'oui', 'oui', 'oui', 'oui', 'oui', 'non'])\n",
    "\n",
    "len(X_jouer), len(Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.1. Probabilité \n",
    "\n",
    "Etant donné une liste des valeurs $S$, la probabilité d'occurence d'une valeur $v$  est le nombre d'occurence de $v$ dans $S$ divisé par le nombre total des éléments de $S$. \n",
    "\n",
    "$$p(v/S) = \\frac{|\\{x / x \\in S \\text{ et } x = v\\}|}{|S|}$$\n",
    "\n",
    "Exemple, prenons la colonne \"jouer\". \n",
    "Le nombre de \"oui\" est 9 et le nombre total est 14. \n",
    "$$p(jouer=oui) = \\frac{9}{14} = 0.6428571428571429$$\n",
    "\n",
    "Voici les paramètres de la fontions :\n",
    "- **S** : un vecteur des valeurs nominales\n",
    "- **v** : une valeur donnée\n",
    "- **résultat** : probabilité d'occurrence de la valeur **v** dans l'ensemble **S**\n",
    "\n",
    "**P.S.** : si la division retourne toujours 0, essayer d'appliquer float(x) sur le numérateur ou le dénominateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6428571428571429,\n",
       " 0.35714285714285715,\n",
       " 0.2857142857142857,\n",
       " 0.35714285714285715)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Probabilité d'occurence d'une valeur dans un ensemble\n",
    "def P(S: np.ndarray, v: str) -> float: \n",
    "    return float((S==v).astype(int).sum())/float((np.shape(S))[0])\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (0.6428571428571429,\n",
    "#  0.35714285714285715,\n",
    "#  0.2857142857142857,\n",
    "#  0.35714285714285715)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "P(Y_jouer      , 'oui'      ), \\\n",
    "P(X_jouer[:, 0], 'ensoleile'), \\\n",
    "P(X_jouer[:, 0], 'nuageux'  ), \\\n",
    "P(X_jouer[:, 0], 'pluvieux')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.2. Incertitude d'un ensemble\n",
    "\n",
    "L'entropie de Shannon correspond à la quantité d'information contenue dans une source d'information ; plus la source émet d'informations différentes, plus l'entropie (ou l'incertitude sur ce que la source émet) est grande.\n",
    "Donc, un ensemble avec une entropie de 0 contient les mêmes valeurs.\n",
    "Etant donné : \n",
    "- $S$ une liste des valeurs \n",
    "- $V$ un ensemble de valeurs uniques de $S$ (vocabulaire) \n",
    "\n",
    "L'entropie de $S$ est calculée comme suit : \n",
    "$$H(S) = - \\sum\\limits_{v \\in V} p(v/S) \\log_2 p(v/S)$$\n",
    "\n",
    "Par exemple, la colonne \"jouer\" contient deux valeurs \"oui\" et \"non\". \n",
    "Son entopie est :\n",
    "$$H(jouer) = - \\frac{9}{14} * \\log_2(\\frac{9}{14}) - \\frac{5}{14} * \\log_2(\\frac{5}{14}) = 0.9402859586706309$$ \n",
    "\n",
    "\n",
    "**P.S.** : np.log2 calcule log2 d'une valeur, vecteur ou matrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9402859586706311, 1.5774062828523454)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Entropie\n",
    "def H(S: np.ndarray) -> float: \n",
    "    V = np.unique(S)\n",
    "    entropie = 0\n",
    "    for i in V:\n",
    "        entropie = entropie + P(S,i) * np.log2(P(S,i))\n",
    "    return -entropie \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (0.9402859586706311, 1.5774062828523454)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "H(Y_jouer), H(X_jouer[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.3. Division d'un ensemble\n",
    "\n",
    "**Rien à programmer ici**\n",
    "\n",
    "Ici, on essaye de diviser la liste des prédictions (classes) selon les valeurs d'un attribut (caractéristique, colonne) à des sous listes. \n",
    "\n",
    "Etant donné : \n",
    "- **Y** : la liste à diviser\n",
    "- **A** : la liste des valeurs d'un attribut (caractéristique, colonne). C'est un vecteur aligné avec Y ; c-à-d, chaque élément de A a un élément de Y respectif.\n",
    "- **v** : la valeur sur laquelle on divie.\n",
    "\n",
    "$$S_{A,v} = \\{y^{(i)} \\in Y / a^{(i)} \\in A \\wedge a^{(i)} = v\\}\\}$$\n",
    "\n",
    "Par exemple, si \n",
    "- $Y$ est la liste des prédictions de \"jouer\"\n",
    "- $A$ est la liste des valeurs de la caractéristique \"temps\"\n",
    "- $v$ est la valeur \"ensoleile\"\n",
    "\n",
    "Le sous ensemble de \"jouer\" où (temps = \"ensoleile\") contient 3 non et 2 oui "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['non', 'non', 'non', 'oui', 'oui'], dtype='<U3')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diviser_ID3(Y: np.ndarray, A: np.ndarray, v: str) -> np.ndarray:\n",
    "    msk = A == v\n",
    "    return Y[msk]\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# array(['non', 'non', 'non', 'oui', 'oui'], dtype='<U3')\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "diviser_ID3(Y_jouer, X_jouer[:,0], 'ensoleile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### I.1.4. Gain d'entropie\n",
    "\n",
    "Le gain d'entropie (information gain) est la différence entre l'entropie avant et après la division d'une liste $Y$ selon l'attribut $A$. \n",
    "En d'autres termes, combien d'incertitude dans $Y$ a été réduite après sa division en utilisant l'attribut $A$.\n",
    "\n",
    "Etant donné : \n",
    "- **Y** : une liste à diviser\n",
    "- **A** : une liste des valeurs d'un attribut (caractéristique, colonne) \n",
    "- **V** : l'ensemble des valeurs différentes de l'attribut A (vocabulaire)\n",
    "- **p(v/A)** : la probabité d'occurence de la valeur $v$ dans $A$\n",
    "- $Y_{A, v}$ : sous-ensemble de $Y$ où les valeurs de $V$ égalent à $v$  en utilisant la fonction précédente (diviser_ID3)\n",
    "\n",
    "Le gain d'entrepie est calculé comme suit : \n",
    "\n",
    "$$IG(Y, A) = H(Y) - \\sum_{v \\in V} p(v/A) H(Y_{A, v})$$\n",
    "\n",
    "Les paramètres de la fonction :\n",
    "- **Y** : un vecteur des valeurs à diviser\n",
    "- **A** : un vecteur d'une caratéristique **A** sur laquelle nous voulons diviser **Y**\n",
    "- **Résultat** : un tuple (gain d'entropie, entropie). On rend l'entropie pour ne pas recalculer ultérierement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24674981977443933, 0.9402859586706311)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Gain d'entropie\n",
    "def IG(Y: np.ndarray, A: np.ndarray) -> Tuple[float, float]:\n",
    "    V = np.unique(A)\n",
    "    entropie = H(Y)\n",
    "    ig_global = entropie\n",
    "    # Compléter ici\n",
    "    for i in V : \n",
    "        ig_global = ig_global - P(A,i)*H(diviser_ID3(Y, A, i))\n",
    "\n",
    "    return ig_global, entropie\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (0.24674981977443933, 0.9402859586706311)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "IG(Y_jouer, X_jouer[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.5. Choix de l'attribut de division ID3\n",
    "\n",
    "Ici, nous devons trouver l'attribut qui maximise IG.\n",
    "\n",
    "$$jj = \\arg\\max_j IG(Y, X_j)$$\n",
    "\n",
    "Les paramètres de la fonction :\n",
    "- **X[M, N]** : une matrice de M échantillons et de N caractéristiques nominales.\n",
    "- **Y[M]** : un vecteur des classes\n",
    "- **Résultat** : un tuple (numéro d'attribut, IG, entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0.24674981977443933, 0.9402859586706311)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Choix d'attribut de dévision\n",
    "def choisir_devision_ID3(X: np.ndarray, Y: np.ndarray) -> Tuple[int, float, float]: \n",
    "    jj = -1 # numéro d'attribut qui maximise IG\n",
    "    ig_jj = -1.0 # IG de cet attribut (le max)\n",
    "    h_jj = -1.0  # Entropie \n",
    "    # Compléter ici\n",
    "    liste = []\n",
    "    for i in range(X.shape[1]) : \n",
    "        liste.append(IG(Y,X[:,i]))\n",
    "    jj = liste.index(max(liste))\n",
    "    ig_jj = IG(Y,X[:,jj])[0]\n",
    "    h_jj =  IG(Y,X[:,jj])[1]\n",
    "\n",
    "    return jj, ig_jj, h_jj\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (0, 0.24674981977443933, 0.9402859586706311)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "choisir_devision_ID3(X_jouer, Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### I.1.6. Arrêt de division\n",
    "\n",
    "Etant donné les données suivantes :\n",
    "- **Y** : l'ensemble des prédiction au niveau d'un noeud\n",
    "- **h** : le critère d'homoginiété. h = 0 ==> l'ensemble Y est homogène (mêmes valeurs)\n",
    "- **nbr_min** : le nombre minimale des observations dans un noeud. |Y| <= nbr_min ==> le noeud doit être une feuille\n",
    "\n",
    "La fonction d'arrêt doit retourner : \n",
    "- Le nom de la classe : si l'ensemble est homogène ou il contient un nombre minimal des éléments\n",
    "- None : sinon\n",
    "\n",
    "**HINT** : utiliser numpy.unique et numpy.argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('oui', 'non', None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Arrêt ID3\n",
    "def classMax(Y: np.ndarray) -> str :\n",
    "    V = np.unique(Y)\n",
    "    freq = []\n",
    "    for i in range(np.shape(V)[0]):   \n",
    "        freq.append(np.shape(np.extract(Y == V[i], Y))[0]) \n",
    "    return V[np.argmax(freq)]\n",
    "\n",
    "def arreter_ID3(Y: np.ndarray, h: float, nbr_min: int) -> Union[str, None]:\n",
    "    if h == 0 :\n",
    "        return Y[0]\n",
    "    else :\n",
    "        if Y.shape[0] <= nbr_min : \n",
    "            return classMax(Y)\n",
    "        else :\n",
    "            return None\n",
    "    return \n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ('oui', 'non', None)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Y_t1 = np.array(['oui', 'oui', 'oui'])\n",
    "Y_t2 = np.array(['oui', 'non', 'non'])\n",
    "\n",
    "arreter_ID3(Y_t1, H(Y_t1), 2), \\\n",
    "arreter_ID3(Y_t2, H(Y_t2), 4), \\\n",
    "arreter_ID3(Y_t2, H(Y_t2), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.7. Création de l'arbre\n",
    "\n",
    "**Rien à programmer ici**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le Code\n",
      "Si X[0] est \"ensoleile\" Alors\n",
      "    Si X[2] est \"haute\" Alors\n",
      "        Y est \"non\"\n",
      "    Si X[2] est \"normale\" Alors\n",
      "        Y est \"oui\"\n",
      "Si X[0] est \"nuageux\" Alors\n",
      "    Y est \"oui\"\n",
      "Si X[0] est \"pluvieux\" Alors\n",
      "    Si X[3] est \"non\" Alors\n",
      "        Y est \"oui\"\n",
      "    Si X[3] est \"oui\" Alors\n",
      "        Y est \"non\"\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'oui'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Une classe pour contenir les informations du noeud \n",
    "# et la liste de ces fils\n",
    "class Noeud(object): \n",
    "    \n",
    "    nbr = 0\n",
    "    \n",
    "    def __init__(self, num:int, ig:float, h:float, profondeur:int): \n",
    "        self.num    = num        # le numéro du caractéristique de dévision dans X\n",
    "        self.ig     = ig         # le IG de division\n",
    "        self.h      = h          # l'entropie H\n",
    "        self.pr     = profondeur # la profondeur du noeud\n",
    "        self.fils   = {}         # les fils ; un dictionnaire valeur : noeud\n",
    "        self.cls    = ''         # la classe si ce noeud est final (s'il n'y a pas de fils)\n",
    "        self.indent = '    '     # indentation lorsqu'on génère le code\n",
    "    \n",
    "    # Cette fonction est pour transformer le noeud à une string\n",
    "    #Ici, nous avons redéfini cette fonction afin qu'elle écrive l'arbre \n",
    "    #sous form d'un algorithme ; c'est un parser \n",
    "    def __str__(self):\n",
    "        \n",
    "        indent = self.indent * self.pr # indentation : esthetique\n",
    "        \n",
    "        # s'il n'y a pas de fils, le noeud est terminal ; on imprime la classe\n",
    "        if (len(self.fils)==0):\n",
    "            return indent + 'Y est \"' + self.cls + '\"\\n'\n",
    "        \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        res = \"\"\n",
    "        for valeur in self.fils:\n",
    "            res += indent + 'Si X[' + str(self.num) + '] est \"' + str(valeur) \n",
    "            res += '\" Alors\\n' + str(self.fils[valeur])\n",
    "        return res\n",
    "    \n",
    "    # predire un échantillon\n",
    "    def predire(self, x: List[str]) -> str: \n",
    "        \n",
    "        # Si le noeud est final, il rend sa classe \n",
    "        if (len(self.fils)==0):\n",
    "            return self.cls\n",
    "        \n",
    "        # Si la valeur de la colonne respective à ce noeud n'appartient pas à l'ensemble des\n",
    "        # valeurs attendues, on rend np.nan\n",
    "        if x[self.num] not in self.fils: \n",
    "            return np.nan\n",
    "        \n",
    "        # Sinon, on rend \n",
    "        return self.fils[x[self.num]].predire(x)\n",
    "    \n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self): \n",
    "        \n",
    "        nid = 'N' + str(Noeud.nbr)\n",
    "        Noeud.nbr += 1\n",
    "        \n",
    "        # Si le noeud est final, \n",
    "        if (len(self.fils)==0):\n",
    "            return nid, nid + '[label=\"' + self.cls + '\" shape=ellipse];\\n'\n",
    "        \n",
    "        # Sinon, \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        res  = nid + '[label=\"X[' + str(self.num) + ']\\\\n'\n",
    "        res += 'H = ' + str(self.h) + '\\\\n'\n",
    "        res += 'IG = ' + str(self.ig) + '\"];\\n'\n",
    "        for valeur in self.fils:\n",
    "            vid, code = self.fils[valeur].graphviz()\n",
    "            res += code\n",
    "            res += nid + ' -> ' + vid + ' [label=\"' + valeur + '\"];\\n'\n",
    "        return nid, res\n",
    "    \n",
    "\n",
    "# créer l'arbre de décision à partir d'un ensemble X et Y\n",
    "def entrainer_ID3(X:np.ndarray, Y:np.ndarray, nbr_min:int=0, profondeur:int=0): \n",
    "    \n",
    "    # Chercher la meilleure caractéristique de X pour diviser Y\n",
    "    jj, ig_jj, h_jj = choisir_devision_ID3(X, Y)\n",
    "    # Créer un noeud\n",
    "    noeud = Noeud(jj, ig_jj, h_jj, profondeur)\n",
    "    # si arrêter rend une classe, donc c'est une feuille \n",
    "    cls = arreter_ID3(Y, h_jj, nbr_min)\n",
    "    if cls:\n",
    "        noeud.cls = cls # la classe du noeud\n",
    "        return noeud # retourner le noeud \n",
    "    \n",
    "    # Sinon, si le noeud n'est pas une feuille, on crée ces fils\n",
    "    profondeur += 1 # la profondeur de ces fils\n",
    "    # les fils sont créés à partir des valeurs uniques du meilleur caractéristique\n",
    "    for v in np.unique(X[:, jj]):\n",
    "        # Ces trois lignes sont pour récupérer les sous-ensembles X_val, Y_val\n",
    "        # Corresondants à une valeur du meilleur caractéristique\n",
    "        msk = X[:, jj] == v \n",
    "        X_v = X[msk]\n",
    "        Y_v = Y[msk]\n",
    "        # On refait la même opération sur l'ensemble (Y_val) d'une manière récursive\n",
    "        fils = entrainer_ID3(X_v, Y_v, nbr_min=nbr_min, profondeur=profondeur)\n",
    "        # On affecte le noeud créé indexé par la valeur du meilleur caractéristique \n",
    "        # à l'ensemble des fils du noeud courant\n",
    "        noeud.fils[v] = fils\n",
    "    \n",
    "    return noeud\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Le Code\n",
    "# Si X[0] est \"ensoleile\" Alors\n",
    "#     Si X[2] est \"haute\" Alors\n",
    "#         Y est \"non\"\n",
    "#     Si X[2] est \"normale\" Alors\n",
    "#         Y est \"oui\"\n",
    "# Si X[0] est \"nuageux\" Alors\n",
    "#     Y est \"oui\"\n",
    "# Si X[0] est \"pluvieux\" Alors\n",
    "#     Si X[3] est \"non\" Alors\n",
    "#         Y est \"oui\"\n",
    "#     Si X[3] est \"oui\" Alors\n",
    "#         Y est \"non\"\n",
    "#\n",
    "# 'oui'\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "arbre_jouer = entrainer_ID3(X_jouer, Y_jouer)\n",
    "\n",
    "print('Le Code')\n",
    "print(arbre_jouer)\n",
    "\n",
    "# Tester sur un échantillon\n",
    "arbre_jouer.predire(['pluvieux', 'temperature_makanche', 'humidite_makanche', 'non'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.8. Regrouper le tous\n",
    "\n",
    "**Rien à programmer ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si temps est \"ensoleile\" Alors\n",
      "    Si humidite est \"haute\" Alors\n",
      "        jouer est \"non\"\n",
      "    Si humidite est \"normale\" Alors\n",
      "        jouer est \"oui\"\n",
      "Si temps est \"nuageux\" Alors\n",
      "    jouer est \"oui\"\n",
      "Si temps est \"pluvieux\" Alors\n",
      "    Si vent est \"non\" Alors\n",
      "        jouer est \"oui\"\n",
      "    Si vent est \"oui\" Alors\n",
      "        jouer est \"non\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ID3(object): \n",
    "    \n",
    "    def entrainer(self, X, Y, X_noms=[], Y_nom='', nbr_min=0):\n",
    "        self.arbre = entrainer_ID3(X, Y, nbr_min=nbr_min)\n",
    "        code = str(self.arbre)\n",
    "        if len(Y_nom) > 0: \n",
    "            code = code.replace('Y', Y_nom)\n",
    "        for i in range(len(X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', X_noms[i])\n",
    "        self.code = code\n",
    "        self.X_noms = X_noms\n",
    "    \n",
    "    def predire(self, X): \n",
    "        predictions = []\n",
    "        for i in range(len(X)): \n",
    "            predictions.append(self.arbre.predire(X[i, :]))\n",
    "        return predictions\n",
    "    \n",
    "    def graphviz(self): \n",
    "        nid, code = self.arbre.graphviz()\n",
    "        res  = 'digraph Tree {\\n'\n",
    "        res += 'node [shape=box] ;'\n",
    "        for i in range(len(self.X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', self.X_noms[i])\n",
    "        res += code\n",
    "        res += '}'\n",
    "        return res\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Si temps est \"ensoleile\" Alors\n",
    "#     Si humidite est \"haute\" Alors\n",
    "#         jouer est \"non\"\n",
    "#     Si humidite est \"normale\" Alors\n",
    "#         jouer est \"oui\"\n",
    "# Si temps est \"nuageux\" Alors\n",
    "#     jouer est \"oui\"\n",
    "# Si temps est \"pluvieux\" Alors\n",
    "#     Si vent est \"non\" Alors\n",
    "#         jouer est \"oui\"\n",
    "#     Si vent est \"oui\" Alors\n",
    "#         jouer est \"non\"\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "id3_classifieur = ID3()\n",
    "id3_classifieur.entrainer(X_jouer, Y_jouer, X_noms=['temps', 'temperature', 'humidite', 'vent'], Y_nom='jouer')\n",
    "print(id3_classifieur.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\anis\\anaconda3\\lib\\site-packages (0.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz\n",
    "# Aussi il faut installer le backend graphviz dans le systeme\n",
    "# Lien : https://graphviz.org/download/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"430pt\" height=\"252pt\" viewBox=\"0.00 0.00 430.00 252.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 248)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-248 426,-248 426,4 -4,4\"/>\n",
       "<!-- N0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>N0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"297.5,-244 124.5,-244 124.5,-191 297.5,-191 297.5,-244\"/>\n",
       "<text text-anchor=\"middle\" x=\"211\" y=\"-228.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">temps</text>\n",
       "<text text-anchor=\"middle\" x=\"211\" y=\"-213.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">H = 0.9402859586706311</text>\n",
       "<text text-anchor=\"middle\" x=\"211\" y=\"-198.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">IG = 0.24674981977443933</text>\n",
       "</g>\n",
       "<!-- N1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>N1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"166,-140 0,-140 0,-87 166,-87 166,-140\"/>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">humidite</text>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-109.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">H = 0.9709505944546686</text>\n",
       "<text text-anchor=\"middle\" x=\"83\" y=\"-94.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">IG = 0.9709505944546686</text>\n",
       "</g>\n",
       "<!-- N0&#45;&gt;N1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>N0-&gt;N1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.7,-190.76C161.87,-177.35 141.14,-160.83 123.27,-146.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"125.29,-143.73 115.29,-140.23 120.93,-149.2 125.29,-143.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"179\" y=\"-161.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">ensoleile</text>\n",
       "</g>\n",
       "<!-- N4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>N4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"211\" cy=\"-113.5\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"211\" y=\"-109.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">oui</text>\n",
       "</g>\n",
       "<!-- N0&#45;&gt;N4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>N0-&gt;N4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M211,-190.76C211,-175.85 211,-157.08 211,-141.9\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"214.5,-141.58 211,-131.58 207.5,-141.58 214.5,-141.58\"/>\n",
       "<text text-anchor=\"middle\" x=\"234\" y=\"-161.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">nuageux</text>\n",
       "</g>\n",
       "<!-- N5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>N5</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"422,-140 256,-140 256,-87 422,-87 422,-140\"/>\n",
       "<text text-anchor=\"middle\" x=\"339\" y=\"-124.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">vent</text>\n",
       "<text text-anchor=\"middle\" x=\"339\" y=\"-109.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">H = 0.9709505944546686</text>\n",
       "<text text-anchor=\"middle\" x=\"339\" y=\"-94.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">IG = 0.9709505944546686</text>\n",
       "</g>\n",
       "<!-- N0&#45;&gt;N5 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>N0-&gt;N5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M243.3,-190.76C260.13,-177.35 280.86,-160.83 298.73,-146.59\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"301.07,-149.2 306.71,-140.23 296.71,-143.73 301.07,-149.2\"/>\n",
       "<text text-anchor=\"middle\" x=\"305.5\" y=\"-161.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">pluvieux</text>\n",
       "</g>\n",
       "<!-- N2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>N2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"47\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"47\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">non</text>\n",
       "</g>\n",
       "<!-- N1&#45;&gt;N2 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>N1-&gt;N2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M73.16,-86.95C68.15,-73.94 62.07,-58.15 57.03,-45.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"60.28,-43.75 53.42,-35.68 53.75,-46.27 60.28,-43.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"82\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">haute</text>\n",
       "</g>\n",
       "<!-- N3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>N3</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"119\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"119\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">oui</text>\n",
       "</g>\n",
       "<!-- N1&#45;&gt;N3 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>N1-&gt;N3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M92.84,-86.95C97.85,-73.94 103.93,-58.15 108.97,-45.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"112.25,-46.27 112.58,-35.68 105.72,-43.75 112.25,-46.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"128.5\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">normale</text>\n",
       "</g>\n",
       "<!-- N6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>N6</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"303\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"303\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">oui</text>\n",
       "</g>\n",
       "<!-- N5&#45;&gt;N6 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>N5-&gt;N6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M329.16,-86.95C324.15,-73.94 318.07,-58.15 313.03,-45.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"316.28,-43.75 309.42,-35.68 309.75,-46.27 316.28,-43.75\"/>\n",
       "<text text-anchor=\"middle\" x=\"333\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">non</text>\n",
       "</g>\n",
       "<!-- N7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>N7</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"375\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"375\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">non</text>\n",
       "</g>\n",
       "<!-- N5&#45;&gt;N7 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>N5-&gt;N7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M348.84,-86.95C353.85,-73.94 359.93,-58.15 364.97,-45.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"368.25,-46.27 368.58,-35.68 361.72,-43.75 368.25,-46.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"370\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">oui</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# C'est juste une visualisation du graphe\n",
    "# Si ça ne marche pas, ce n'ai pas grave\n",
    "try:\n",
    "    from IPython.display import SVG\n",
    "    from graphviz        import Source\n",
    "    from IPython.display import display\n",
    "    \n",
    "    graph = Source(id3_classifieur.graphviz())\n",
    "    display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "except ImportError:\n",
    "    print('il faut installer graphviz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. CART\n",
    "\n",
    "Ici, nous allons implémenter l'algorithme CART pour la classification avec des caractéristiques numériques. \n",
    "Nous allons utiliser le dataset \"jouer (numériques)\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# temperature, humidite, vent\n",
    "X_njouer = np.array([\n",
    "    [30, 85, 0],\n",
    "    [27, 90, 1],\n",
    "    [28, 78, 0],\n",
    "    [21, 96, 0],\n",
    "    [20, 80, 0],\n",
    "    [18, 70, 1],\n",
    "    [18, 65, 1],\n",
    "    [22, 95, 0],\n",
    "    [21, 70, 0],\n",
    "    [24, 80, 0],\n",
    "    [24, 70, 1],\n",
    "    [22, 90, 1],\n",
    "    [27, 75, 0],\n",
    "    [22, 80, 1]\n",
    "])\n",
    "\n",
    "Y_njouer = Y_jouer \n",
    "\n",
    "len(X_njouer), len(Y_njouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.1. Index de diversité de Gini\n",
    "\n",
    "Dans le cas de classement, CART utilise l'indexe de diversité Gini pour mesurer l'erreur de classification.\n",
    "Un index de 0 représente la meilleure division; \n",
    "\n",
    "Etant donné : \n",
    "- $S$ liste des valeurs  \n",
    "- $V$ ensemble des valeurs uniques de $S$  (vocabulaire)\n",
    "\n",
    "L'index de diversité  $Gini(S)$ est calculée comme suit : \n",
    "$$Gini(S) = \\sum\\limits_{v \\in V} p(v/S) (1-p(v/S)) = 1 - \\sum\\limits_{v \\in V} p(v/S)^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4591836734693877"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Gini\n",
    "def Gini(S: np.ndarray) -> float:  \n",
    "    V = np.unique(S)\n",
    "    gini = 0\n",
    "    # Compléter ici\n",
    "    for i in V:\n",
    "        gini+=P(S,i)*(1-P(S,i))\n",
    "    return gini \n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : 0.4591836734693877\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Gini(Y_njouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Division d'un ensemble\n",
    "\n",
    "**Rien à programmer ici**\n",
    "\n",
    "Ici, nous essayons de diviser la liste des prédictions (classes) $Y$ selon une valeur donnée $v$ d'un attribut (caractéristique, colonne) $A$ sur deux listes :\n",
    "- $Y_G$ : une liste contenant les éléments de $Y$ où $A > v$\n",
    "- $Y_D$ : une liste contenant les éléments de $Y$ où $A \\le v$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['non', 'non', 'oui', 'oui', 'non', 'oui', 'oui', 'oui', 'oui',\n",
       "        'oui', 'non'], dtype='<U3'),\n",
       " array(['oui', 'non', 'oui'], dtype='<U3'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diviser_CART(Y:np.ndarray, A:np.ndarray, v:float):\n",
    "    msk = A > v\n",
    "    return Y[msk], Y[~msk]\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (array(['non', 'non', 'oui', 'oui', 'non', 'oui', 'oui', 'oui', 'oui',\n",
    "#         'oui', 'non'], dtype='<U3'),\n",
    "#  array(['oui', 'non', 'oui'], dtype='<U3'))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "diviser_CART(Y_njouer, X_njouer[:,0], 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3. Diversité Gini de la division\n",
    "\n",
    "Etant donné : \n",
    "- **Y** : une liste de prédictions\n",
    "- **V** : les différentes valeurs de Y (les classes\n",
    "- S_G, G_D : sous ensembles gauche et droit\n",
    "- $|S| = |S_G| + |S_D|$\n",
    "\n",
    "La diversité Gini de la division : \n",
    "\n",
    "$$Gini_{div}(S_G, S_D) = \\frac{|S_G|}{|S|} Gini(S_G) + \\frac{|S_D|}{|S|} Gini(S_D)$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4588744588744589"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Diversité gini de la division\n",
    "def Gini_div(S_G: np.ndarray, S_D: np.ndarray) -> float:  \n",
    "    S_len = float(len(S_G) + len(S_D)) \n",
    "    # Compléter ici\n",
    "    return len(S_G)/S_len *Gini(S_G) + len(S_D)/S_len *Gini(S_D)\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# 0.4588744588744589\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "S_G, S_D = diviser_CART(Y_njouer, X_njouer[:, 0], 20)\n",
    "Gini_div(S_G, S_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.4. Choix de l'attribut et la valeur de division CART\n",
    "\n",
    "L'algorithme\n",
    "- Pour chaque ccaractéristique $X_j$ \n",
    "   - Pour chaque valeur $v$ appartennant aux valeurs uniques de $X_j$\n",
    "       1. Diviser $Y$ en se basant sur la valeur $v$ et celles de $X_j$\n",
    "       1. Calculer Gini de cette division \n",
    "       1. Garder l'indice **jj** de la caractéristique qui minimise Gini\n",
    "       1. Garder Gini minimale **gini_jj**\n",
    "       1. Garder la valeur de dévision **v_jj**\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0.3936507936507937, 80)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Choix de l'attribut et la valeur de division CART\n",
    "def choisir_division_CART(X: np.ndarray, Y: np.ndarray) -> Tuple[int, float, float]: \n",
    "    jj      = -1\n",
    "    gini_jj =  1.0\n",
    "    v_jj    = -1.0\n",
    "    # Compléter ici\n",
    "    for indice in range(0,X.shape[1]):\n",
    "        V=np.unique(X[:,indice])\n",
    "        for v in V:\n",
    "              G,D=diviser_CART(Y, X[:,indice], v)\n",
    "              val=Gini_div(G,D)\n",
    "              if(val<gini_jj):\n",
    "                jj=indice\n",
    "                gini_jj=val\n",
    "                v_jj=v\n",
    "    return jj, gini_jj, v_jj\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# (1, 0.3936507936507937, 80)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "choisir_division_CART(X_njouer, Y_njouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.5. Arrêt de division CART\n",
    "\n",
    "**Rien à programmer ici** \n",
    "\n",
    "On va utiliser la même fonction que celle de ID3. Mais, pour être consistant, on va seulement renomer la fonction et passer Gini à la place de l'entropie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('oui', 'non', None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arreter_CART = arreter_ID3\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# ('oui', 'non', None)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Y_t1 = np.array(['oui', 'oui', 'oui'])\n",
    "Y_t2 = np.array(['oui', 'non', 'non'])\n",
    "arreter_CART(Y_t1, Gini(Y_t1), 2), \\\n",
    "arreter_CART(Y_t2, Gini(Y_t2), 4), \\\n",
    "arreter_CART(Y_t2, Gini(Y_t2), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.6. Création de l'arbre\n",
    "\n",
    "**Rien à programmer ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le Code\n",
      "Si X[1]  > 80 Alors\n",
      "    Si X[0]  > 22 Alors\n",
      "        Y est \"non\"\n",
      "    Sinon\n",
      "        Si X[0]  > 21 Alors\n",
      "            Y est \"non\"\n",
      "        Sinon\n",
      "            Y est \"oui\"\n",
      "Sinon\n",
      "    Si X[2]  > 0 Alors\n",
      "        Si X[0]  > 22 Alors\n",
      "            Y est \"oui\"\n",
      "        Sinon\n",
      "            Y est \"non\"\n",
      "    Sinon\n",
      "        Y est \"oui\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comme Noeud, mais il faut changer un peu, puisqu'on teste sur des \n",
    "# valeurs numériques aussi\n",
    "# Le code sera plus utilisable si on crée une classe commune et on hérite\n",
    "# Mais, je n'ai pas le temps pour tout ça (DEAL WITH IT)\n",
    "class NoeudBin(object): \n",
    "    \n",
    "    nbr = 0\n",
    "    \n",
    "    def __init__(self, num:int, val:int, gini:float, profondeur:int): \n",
    "        self.num    = num        # le numéro du caractéristique de dévision dans X\n",
    "        self.val    = val        # la valeur du noeud\n",
    "        self.gini   = gini       # le Gini de division\n",
    "        self.pr     = profondeur # la profondeur du noeud\n",
    "        self.fils   = []         # les fils ; un tableau de deux noeuds: S_G, S_D\n",
    "        self.cls    = ''         # la classe si ce noeud est final (s'il n'y a pas de fils)\n",
    "        self.indent = '    '     # indentation lorsqu'on génère le code\n",
    "    \n",
    "    # Cette fonction est pour transformer le noeud à une string\n",
    "    #Ici, nous avons redéfini cette fonction afin qu'elle écrive l'arbre \n",
    "    #sous form d'un algorithme ; c'est un parser \n",
    "    def __str__(self):\n",
    "        \n",
    "        indent = self.indent * self.pr # indentation : esthetique\n",
    "        \n",
    "        # s'il n'y a pas de fils, le noeud est terminal ; on imprime la classe\n",
    "        if (len(self.fils)==0):\n",
    "            return indent + 'Y est \"' + self.cls + '\"\\n'\n",
    "         \n",
    "        prefix = ' > '\n",
    "        suffix = ''\n",
    "        \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS SINON\n",
    "        res  = ''\n",
    "        res += indent + 'Si X[' + str(self.num) + '] ' + prefix + str(self.val) + suffix \n",
    "        res += ' Alors\\n' + str(self.fils[0])\n",
    "        res += indent + 'Sinon\\n' + str(self.fils[1])\n",
    "        return res\n",
    "    \n",
    "    # predire un échantillon\n",
    "    def predire(self, x: List[float]): \n",
    "        \n",
    "        # Si le noeud est final, il rend sa classe \n",
    "        if (len(self.fils)==0):\n",
    "            return self.cls\n",
    "        \n",
    "        # sinon\n",
    "        if x[self.num] > self.val:\n",
    "            return self.fils[0].predire(x)\n",
    "        return self.fils[1].predire(x)\n",
    "\n",
    "    \n",
    "    # générer un code pour graphviz\n",
    "    def graphviz(self): \n",
    "        \n",
    "        nid = 'N' + str(NoeudBin.nbr)\n",
    "        NoeudBin.nbr += 1\n",
    "        \n",
    "        # Si le noeud est final, \n",
    "        if (len(self.fils)==0):\n",
    "            return nid, nid + '[label=\"' + self.cls + '\" shape=ellipse];\\n'\n",
    "        \n",
    "        # Sinon, \n",
    "        # s'il y a des fils, on boucle sur les fils et on imprime des SI ... ALORS\n",
    "        prefix = '] > '\n",
    "        res = nid + '[label=\"X[' + str(self.num) + prefix + str(self.val) + '\\\\n'\n",
    "        res += 'Gini = ' + str(self.gini) + '\"];\\n'\n",
    "        vid_G, code_G = self.fils[0].graphviz()\n",
    "        vid_D, code_D = self.fils[1].graphviz()\n",
    "        \n",
    "        res += code_G + code_D\n",
    "        res += nid + ' -> ' + vid_G + ' [label=\"Vrai\"];\\n'\n",
    "        res += nid + ' -> ' + vid_D + ' [label=\"Faux\"];\\n'\n",
    "        return nid, res\n",
    "\n",
    "# créer l'arbre de décision à partir d'un ensemble X et Y\n",
    "def entrainer_CART(X:np.ndarray, Y:np.ndarray, profondeur:int=0, nbr_min:int=0): \n",
    "    \n",
    "    # Chercher le meilleur caractéristique de X pour diviser Y\n",
    "    jj, gini_jj, v_jj = choisir_division_CART(X, Y)\n",
    "    # Créer un noeud\n",
    "    noeud = NoeudBin(jj, v_jj, gini_jj, profondeur)\n",
    "    # Si l'entropie est 0 donc le noeud est terminal, élagage\n",
    "    \n",
    "    cls = arreter_CART(Y, gini_jj, nbr_min)\n",
    "    if cls:\n",
    "        noeud.cls = cls # la classe du noeud\n",
    "        return noeud # retourner le noeud \n",
    "     \n",
    "    \n",
    "    # Sinon, si le noeud n'est pas terminal, on crée ces fils\n",
    "    profondeur += 1 # la profondeur de ces fils\n",
    "    # création des deux fils\n",
    "    \n",
    "    msk = X[:, jj] > v_jj\n",
    "    \n",
    "    X_G = X[msk]\n",
    "    Y_G = Y[msk]\n",
    "    fils_G = entrainer_CART(X_G, Y_G, profondeur, nbr_min)\n",
    "    X_D = X[~msk]\n",
    "    Y_D = Y[~msk]\n",
    "    fils_D = entrainer_CART(X_D, Y_D, profondeur, nbr_min)\n",
    "    noeud.fils.append(fils_G)\n",
    "    noeud.fils.append(fils_D)\n",
    "    \n",
    "    return noeud\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Le Code\n",
    "# Si X[1]  > 80 Alors\n",
    "#     Si X[0]  > 22 Alors\n",
    "#         Y est \"non\"\n",
    "#     Sinon\n",
    "#         Si X[0]  > 21 Alors\n",
    "#             Y est \"non\"\n",
    "#         Sinon\n",
    "#             Y est \"oui\"\n",
    "# Sinon\n",
    "#     Si X[2]  > 0 Alors\n",
    "#         Si X[1]  > 70 Alors\n",
    "#             Y est \"non\"\n",
    "#         Sinon\n",
    "#             Si X[0]  > 18 Alors\n",
    "#                 Y est \"oui\"\n",
    "#             Sinon\n",
    "#                 Y est \"non\"\n",
    "#     Sinon\n",
    "#         Y est \"oui\"\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "arbre_njouer = entrainer_CART(X_njouer, Y_njouer)\n",
    "\n",
    "print('Le Code')\n",
    "print(arbre_njouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.7. Regrouper le tous\n",
    "\n",
    "**Rien à programmer ici**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Si humidite  > 80 Alors\n",
      "    Si temperature  > 22 Alors\n",
      "        jouer est \"non\"\n",
      "    Sinon\n",
      "        Si temperature  > 21 Alors\n",
      "            jouer est \"non\"\n",
      "        Sinon\n",
      "            jouer est \"oui\"\n",
      "Sinon\n",
      "    Si vent  > 0 Alors\n",
      "        Si temperature  > 22 Alors\n",
      "            jouer est \"oui\"\n",
      "        Sinon\n",
      "            jouer est \"non\"\n",
      "    Sinon\n",
      "        jouer est \"oui\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class CART(object): \n",
    "    \n",
    "    def entrainer(self, X:np.ndarray, Y:np.ndarray, X_noms:List[str]=[], Y_nom:str='', nbr_min:int=0):\n",
    "        self.arbre = entrainer_CART(X, Y, 0, nbr_min)\n",
    "        code = str(self.arbre)\n",
    "        if len(Y_nom) > 0: \n",
    "            code = code.replace('Y', Y_nom)\n",
    "        for i in range(len(X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', X_noms[i])\n",
    "        self.code   = code\n",
    "        self.X_noms = X_noms\n",
    "    \n",
    "    def predire(self, X:np.ndarray): \n",
    "        predictions = []\n",
    "        for i in range(len(X)): \n",
    "            predictions.append(self.arbre.predire(X[i, :]))\n",
    "        return predictions\n",
    "    \n",
    "    def graphviz(self): \n",
    "        nid, code = self.arbre.graphviz()\n",
    "        res  = 'digraph Tree {\\n'\n",
    "        res += 'node [shape=box] ;'\n",
    "        for i in range(len(self.X_noms)): \n",
    "            code = code.replace('X[' + str(i) + ']', self.X_noms[i])\n",
    "        res += code\n",
    "        res += '}'\n",
    "        return res\n",
    "\n",
    "#=====================================================================\n",
    "# TEST UNITAIRE\n",
    "#=====================================================================\n",
    "# Resultat : \n",
    "# Si humidite  > 80 Alors\n",
    "#     Si temperature  > 22 Alors\n",
    "#         jouer est \"non\"\n",
    "#     Sinon\n",
    "#         Si temperature  > 21 Alors\n",
    "#             jouer est \"non\"\n",
    "#         Sinon\n",
    "#             jouer est \"oui\"\n",
    "# Sinon\n",
    "#     Si vent  > 0 Alors\n",
    "#         Si humidite  > 70 Alors\n",
    "#             jouer est \"non\"\n",
    "#         Sinon\n",
    "#             Si temperature  > 18 Alors\n",
    "#                 jouer est \"oui\"\n",
    "#             Sinon\n",
    "#                 jouer est \"non\"\n",
    "#     Sinon\n",
    "#         jouer est \"oui\"\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "cart_classifieur = CART()\n",
    "cart_classifieur.entrainer(X_njouer, Y_njouer, X_noms=['temperature', 'humidite', 'vent'], Y_nom='jouer')\n",
    "print(cart_classifieur.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"526pt\" height=\"311pt\" viewBox=\"0.00 0.00 526.00 311.00\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 307)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-307 522,-307 522,4 -4,4\"/>\n",
       "<!-- N0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>N0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"343,-303 169,-303 169,-265 343,-265 343,-303\"/>\n",
       "<text text-anchor=\"middle\" x=\"256\" y=\"-287.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">humidite &gt; 80</text>\n",
       "<text text-anchor=\"middle\" x=\"256\" y=\"-272.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gini = 0.3936507936507937</text>\n",
       "</g>\n",
       "<!-- N1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>N1</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"249.5,-214 68.5,-214 68.5,-176 249.5,-176 249.5,-214\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-198.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">temperature &gt; 22</text>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-183.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gini = 0.26666666666666666</text>\n",
       "</g>\n",
       "<!-- N0&#45;&gt;N1 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>N0-&gt;N1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M235.9,-264.97C221.73,-252.26 202.55,-235.06 186.89,-221.02\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"188.85,-218.07 179.07,-214 184.18,-223.29 188.85,-218.07\"/>\n",
       "<text text-anchor=\"middle\" x=\"226\" y=\"-235.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>N6</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"442,-214 268,-214 268,-176 442,-176 442,-214\"/>\n",
       "<text text-anchor=\"middle\" x=\"355\" y=\"-198.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">vent &gt; 0</text>\n",
       "<text text-anchor=\"middle\" x=\"355\" y=\"-183.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gini = 0.2222222222222222</text>\n",
       "</g>\n",
       "<!-- N0&#45;&gt;N6 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>N0-&gt;N6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276.51,-264.97C291.11,-252.14 310.91,-234.74 326.97,-220.63\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"329.31,-223.23 334.51,-214 324.69,-217.98 329.31,-223.23\"/>\n",
       "<text text-anchor=\"middle\" x=\"326\" y=\"-235.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "<!-- N2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>N2</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-106\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-102.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">non</text>\n",
       "</g>\n",
       "<!-- N1&#45;&gt;N2 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>N1-&gt;N2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M131.65,-175.97C108.98,-161.03 76.93,-139.91 54.28,-124.98\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"56.08,-121.97 45.8,-119.39 52.23,-127.82 56.08,-121.97\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-146.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>N3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"246,-125 72,-125 72,-87 246,-87 246,-125\"/>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-109.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">temperature &gt; 21</text>\n",
       "<text text-anchor=\"middle\" x=\"159\" y=\"-94.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gini = 0.3333333333333333</text>\n",
       "</g>\n",
       "<!-- N1&#45;&gt;N3 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>N1-&gt;N3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M159,-175.97C159,-164.19 159,-148.56 159,-135.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"162.5,-135 159,-125 155.5,-135 162.5,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"173\" y=\"-146.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "<!-- N4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>N4</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"113\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"113\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">non</text>\n",
       "</g>\n",
       "<!-- N3&#45;&gt;N4 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>N3-&gt;N4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M149.24,-86.76C142.6,-74.34 133.72,-57.74 126.42,-44.08\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"129.46,-42.35 121.66,-35.18 123.29,-45.65 129.46,-42.35\"/>\n",
       "<text text-anchor=\"middle\" x=\"151\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>N5</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"185\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"185\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">oui</text>\n",
       "</g>\n",
       "<!-- N3&#45;&gt;N5 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>N3-&gt;N5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M164.51,-86.76C168.15,-74.74 172.97,-58.8 177.02,-45.4\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"180.39,-46.34 179.93,-35.75 173.69,-44.31 180.39,-46.34\"/>\n",
       "<text text-anchor=\"middle\" x=\"189\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "<!-- N7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>N7</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"445.5,-125 264.5,-125 264.5,-87 445.5,-87 445.5,-125\"/>\n",
       "<text text-anchor=\"middle\" x=\"355\" y=\"-109.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">temperature &gt; 22</text>\n",
       "<text text-anchor=\"middle\" x=\"355\" y=\"-94.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Gini = 0.33333333333333337</text>\n",
       "</g>\n",
       "<!-- N6&#45;&gt;N7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>N6-&gt;N7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M355,-175.97C355,-164.19 355,-148.56 355,-135.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"358.5,-135 355,-125 351.5,-135 358.5,-135\"/>\n",
       "<text text-anchor=\"middle\" x=\"367\" y=\"-146.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>N10</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"491\" cy=\"-106\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"491\" y=\"-102.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">oui</text>\n",
       "</g>\n",
       "<!-- N6&#45;&gt;N10 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>N6-&gt;N10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M383.18,-175.97C406.67,-160.95 439.95,-139.66 463.3,-124.72\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"465.5,-127.47 472.03,-119.13 461.72,-121.57 465.5,-127.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"446\" y=\"-146.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "<!-- N8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>N8</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"319\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"319\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">oui</text>\n",
       "</g>\n",
       "<!-- N7&#45;&gt;N8 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>N7-&gt;N8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M347.37,-86.76C342.25,-74.54 335.44,-58.27 329.78,-44.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"332.98,-43.34 325.89,-35.47 326.53,-46.04 332.98,-43.34\"/>\n",
       "<text text-anchor=\"middle\" x=\"352\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Vrai</text>\n",
       "</g>\n",
       "<!-- N9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>N9</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"391\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"391\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">non</text>\n",
       "</g>\n",
       "<!-- N7&#45;&gt;N9 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>N7-&gt;N9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M362.63,-86.76C367.75,-74.54 374.56,-58.27 380.22,-44.74\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"383.47,-46.04 384.11,-35.47 377.02,-43.34 383.47,-46.04\"/>\n",
       "<text text-anchor=\"middle\" x=\"391\" y=\"-57.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Faux</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# C'est juste une visualisation du graphe\n",
    "# Si ça ne marche pas, ce n'ai pas grave\n",
    "try:\n",
    "    from IPython.display import SVG\n",
    "    from graphviz import Source\n",
    "    from IPython.display import display\n",
    "    \n",
    "    graph = Source(cart_classifieur.graphviz())\n",
    "    display(SVG(graph.pipe(format='svg')))\n",
    "\n",
    "except ImportError:\n",
    "    print('il faut installer graphviz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application et analyse\n",
    "\n",
    "Nous allons utiliser le dataset [Cars Data](https://www.kaggle.com/abineshkumark/carsdata) pour classer les voitures en trois classes : US., Euroupe. ou Japan. \n",
    "Dans cette section, nous allons discuter les paramètres des arbres de décision et les forêts aléatoires implémentés dans des outils connus (dans ce cas, scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>cubicinches</th>\n",
       "      <th>hp</th>\n",
       "      <th>weightlbs</th>\n",
       "      <th>time-to-60</th>\n",
       "      <th>year</th>\n",
       "      <th>brand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165</td>\n",
       "      <td>4209.0</td>\n",
       "      <td>12</td>\n",
       "      <td>1972</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.9</td>\n",
       "      <td>4</td>\n",
       "      <td>89.0</td>\n",
       "      <td>71</td>\n",
       "      <td>1925.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1980</td>\n",
       "      <td>Europe.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140</td>\n",
       "      <td>3449.0</td>\n",
       "      <td>11</td>\n",
       "      <td>1971</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>400.0</td>\n",
       "      <td>150</td>\n",
       "      <td>3761.0</td>\n",
       "      <td>10</td>\n",
       "      <td>1971</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30.5</td>\n",
       "      <td>4</td>\n",
       "      <td>98.0</td>\n",
       "      <td>63</td>\n",
       "      <td>2051.0</td>\n",
       "      <td>17</td>\n",
       "      <td>1978</td>\n",
       "      <td>US.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  cubicinches   hp  weightlbs  time-to-60  year    brand\n",
       "0  14.0          8        350.0  165     4209.0          12  1972      US.\n",
       "1  31.9          4         89.0   71     1925.0          14  1980  Europe.\n",
       "2  17.0          8        302.0  140     3449.0          11  1971      US.\n",
       "3  15.0          8        400.0  150     3761.0          10  1971      US.\n",
       "4  30.5          4         98.0   63     2051.0          17  1978      US."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lecture du dataset\n",
    "cars_data = pd.read_csv('data/cars.csv', skipinitialspace=True)\n",
    "# On a remarqué que le type de cette caractéristique n'est pas bien détecté\n",
    "# cars_data['cubicinches'] = pd.to_numeric(cars_data['cubicinches'])\n",
    "# supprimer les valeurs absentes \n",
    "cars_data.dropna(inplace=True)\n",
    "# Yay! We did it! Voici les premières lignes du dataset\n",
    "cars_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mpg            float64\n",
       "cylinders        int64\n",
       "cubicinches    float64\n",
       "hp               int64\n",
       "weightlbs      float64\n",
       "time-to-60       int64\n",
       "year             int64\n",
       "brand           object\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cars_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((204, 7), (52, 7))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_cars = cars_data.values[:, :-1]\n",
    "Y_cars = cars_data.values[:,  -1]\n",
    "\n",
    "X_cars_train, X_cars_test, Y_cars_train, Y_cars_test = train_test_split(X_cars, \n",
    "                                                                        Y_cars, \n",
    "                                                                        test_size=0.2, \n",
    "                                                                        random_state=0) \n",
    "\n",
    "X_cars_train.shape, X_cars_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Arbres de décision\n",
    "\n",
    "Dans l'implémentation Scikit-learn des arbres de décision (**DecisionTreeClassifier**), les caractéristiques sont permutées d'une façon aléatoire à chaque division. \n",
    "Ceci rendre l'arbre non déterministe. \n",
    "Pour arrêter ça, la proporiété **random_state=0** est utilisée.\n",
    "\n",
    "#### II.1.1. Critère de choix des caractéristiques\n",
    "\n",
    "Nous avons entraîné deux arbres de décision CART avec les critères de division : \n",
    "1. Entropy\n",
    "1. Gini\n",
    "\n",
    "Dans scikit-learn, il n'existe que CART (les arbres sont binaires). \n",
    "Mais le critère de division peut être choisi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin\n"
     ]
    }
   ],
   "source": [
    "from   sklearn.tree    import DecisionTreeClassifier\n",
    "from   sklearn.metrics import f1_score\n",
    "from   sklearn         import tree\n",
    "import timeit\n",
    "\n",
    "Xchoix_train = X_cars_train\n",
    "Ychoix_train = Y_cars_train\n",
    "Xchoix_test  = X_cars_test\n",
    "Ychoix_test  = Y_cars_test\n",
    "fnames       = cars_data.columns\n",
    "\n",
    "#Xchoix_train = X_njouer[4:, :]\n",
    "#Ychoix_train = Y_njouer[4:   ]\n",
    "#Xchoix_test  = X_njouer[:4, :]\n",
    "#Ychoix_test  = Y_njouer[:4   ]\n",
    "#fnames       = ['temperature', 'humidite', 'vent']\n",
    "\n",
    "\n",
    "gini_stats          = []\n",
    "entropy_stats       = []\n",
    "gini_classifieur    = DecisionTreeClassifier(criterion='gini'   , random_state=0)\n",
    "entropy_classifieur = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "\n",
    "# ============ GINI ====================\n",
    "# ............ Entraînement ............\n",
    "temps_debut = timeit.default_timer()\n",
    "gini_classifieur.fit(Xchoix_train, Ychoix_train)\n",
    "gini_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ..... Evaluation entrainement ........\n",
    "gini_stats.append(f1_score(Ychoix_train, gini_classifieur.predict(Xchoix_train), average='micro'))\n",
    "# ................ Test ................\n",
    "temps_debut = timeit.default_timer()\n",
    "Ychoix_pred = gini_classifieur.predict(Xchoix_test)\n",
    "gini_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ........... Evaluation test ...........\n",
    "gini_stats.append(f1_score(Ychoix_test, Ychoix_pred, average='micro'))\n",
    "\n",
    "# =========== Entropy ==================\n",
    "# ............ Entraînement ............\n",
    "temps_debut = timeit.default_timer()\n",
    "entropy_classifieur.fit(Xchoix_train, Ychoix_train)\n",
    "entropy_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ..... Evaluation entrainement ........\n",
    "entropy_stats.append(f1_score(Ychoix_train, entropy_classifieur.predict(Xchoix_train), average='micro'))\n",
    "# ................ Test ................\n",
    "temps_debut = timeit.default_timer()\n",
    "Ychoix_pred = entropy_classifieur.predict(Xchoix_test)\n",
    "entropy_stats.append(timeit.default_timer() - temps_debut)\n",
    "# ........... Evaluation test ...........\n",
    "entropy_stats.append(f1_score(Ychoix_test, Ychoix_pred, average='micro'))\n",
    "\n",
    "print('Fin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2)\n",
    "fig.set_figwidth (20)\n",
    "fig.set_figheight(12)\n",
    "tree.plot_tree(entropy_classifieur, ax=ax1, feature_names=fnames, filled=True)\n",
    "tree.plot_tree(gini_classifieur   , ax=ax2, feature_names=fnames, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    'Criteres' : ['Temps Entrainement', 'F1 Entrainement', 'Temps Test', 'F1 Test'],\n",
    "    'Entropie' : entropy_stats,\n",
    "    'Gini'     : gini_stats\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous concernant la structure des deux arbres : ENTOPY (à gauche) et GINI (à droit) ? La structure veut dire : la profendeur, le nombre des feuilles, la position des feuilles (près de la racine ou non), etc.\n",
    "- Justifier le temps d'entraînement et de test en se basant sur les deux algorithmes et les deux structures discutées.\n",
    "\n",
    "**Réponse**\n",
    "- La profondeur de ENTOPY = 15 et celle de GINI = 17 et le nombre de feuilles est identique = 32, les feuilles d'entropy sont plus proche au racine.\n",
    "- Le temps d'entrainement de ENTROPY est plus grand que celui du GINI parce que elle utilise des logarithmes (log est tres couteux) et, par conséquent, le calcul de l'indice de Gini sera plus rapide.\n",
    "- Le temps de test et de Entropie est toujours moins que celui de Gini est cela est justifier par la profondeur de l'arbre de decision qui est plus petit pour ENTROPY donc la decision est plus rapide que Gini et meme les feuilles sont plus proche de la racine dans le cas du ENTROPY. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.2. Profondeur maximale de l'arbre\n",
    "\n",
    "Pour chaque profondeur, nous entraînons un modèle et nous mesurons sa convergence et sa performance en terme du score F1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_PROF     = 40\n",
    "MAX_PROF_lst = range(1, MAX_PROF+1, 1)\n",
    "\n",
    "f1_train_pm  = []\n",
    "f1_test_pm   = []\n",
    "for max_prof in MAX_PROF_lst:\n",
    "    classifieur = DecisionTreeClassifier(random_state=0, max_depth=max_prof)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_pm.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_pm.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MAX_PROF_lst, f1_train_pm, color='blue', label='Convergence (entrainement)')\n",
    "plt.plot(MAX_PROF_lst, f1_test_pm , color='red' , label='Generalisation (Test)'     )\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Profondeur max de l\\'arbre')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Est-ce que plus de profondeur veut dire le modèle va généraliser mieux ?\n",
    "- Justifier\n",
    "\n",
    "**Réponse**\n",
    "- On remarque que l'auguementation de la profondeur mene a une meilleure generalisation (F1 score) mais apres une certaine profondeur la convergence et generalisation se stabilisent.\n",
    "- Oui mais a certain seuil il peut mener a un overfitting.\n",
    "- Pour l'entrainement si on augmente max_depth, F1 augmentera toujours ( et se stabilise a 1, pas de diminution), mais pour le test si on mis max_depth trés élevé, l'arbre de décision peut sur-ajuster les données d'entrainement, cela entraînera une augmentation de l'erreur de test donc pour il faut trouver un compromis biais-varience en choisisant la bonne profondeur maximale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.1.3. Observations minimales dans les feuilles\n",
    "\n",
    "Pour chaque nombre des observations minimales dans les feuilles, nous entraînons un modèle et nous mesurons sa convergence et sa performance en terme du score F1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FEUILLE     = 40\n",
    "MIN_FEUILLE_lst = range(1, MIN_FEUILLE+1, 1)\n",
    "\n",
    "f1_train_mf     = []\n",
    "f1_test_mf      = []\n",
    "for min_feuille in MIN_FEUILLE_lst:\n",
    "    classifieur = DecisionTreeClassifier(random_state=0, min_samples_leaf=min_feuille)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_mf.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_mf.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MIN_FEUILLE_lst, f1_train_mf, color='blue', label='Convergence (entrainement)')\n",
    "plt.plot(MIN_FEUILLE_lst, f1_test_mf , color='red' , label='Generalisation (Test)'     )\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Observations minimales dans les feuilles')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Justifier pourquoi la performance se diminue (en indiquant si ce critère d'arrêt garantie un apprentissage normale ou peut causer un sur-apprentissage ou sous-apprentissage)\n",
    "\n",
    "**Réponse**\n",
    "- La performance diminue quand le nombre d'observations minimales dans les feuilles devienne important\n",
    "- un sous-apprentissage est provoqué si le nombre d'observations minimales est tres **grand** (i.e prendre en compte de toutes les observations à un seul nœud), et un sur-apprentissage si il est tres **petit** (i.e prendre en compte d'au moins un échantillon à chaque nœud ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### II.2. Forêts aléatoires\n",
    "\n",
    "#### II.2.1. Nombre des arbres\n",
    "\n",
    "Pour chaque nombre des arbres dans le forêt, nous entraînons un modèle et nous mesurons sa convergence et sa performance en terme du score F1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "MAX_ARBRE     = 100\n",
    "MAX_ARBRE_lst = range(1, MAX_ARBRE+1, 1)\n",
    "\n",
    "f1_train_nbarbres = []\n",
    "f1_test_nbarbres  = []\n",
    "for max_arbres in MAX_ARBRE_lst:\n",
    "    classifieur = RandomForestClassifier(n_estimators=max_arbres)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_nbarbres.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_nbarbres.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MAX_ARBRE_lst, f1_train_nbarbres, color='blue', label='Convergence  (entrainement)')\n",
    "plt.plot(MAX_ARBRE_lst, f1_test_nbarbres , color='red' , label='Generalisation (Test)'      )\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Nombre des arbres')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ? (convergence et généralisation)\n",
    "- Est-ce que plus d'arbre PEUT améliorer la performance ? Expliquer.\n",
    "\n",
    "**Réponse**\n",
    "- On remarque une augmentation du score F1 de la convergence en augmentant le nombre des arbres mais se stabilise a 1 a partir d'un certain seuil, meme chose pour la generalisation mais on remarque que le score F1 n'est plus stable (des oscillations). \n",
    "\n",
    "- Plus d'arbre peut améliorer la performance en faisant un compromis avec le nombre de caractéristiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.2. Profondeur maximale des arbres\n",
    "\n",
    "Pour chaque profondeur, nous entraînons un forêt et nous mesurons sa convergence et sa performance en terme du score F1. \n",
    "Nous comparons les résultats avec les arbres équivalents (avec la même profondeur).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train_pm_foret = []\n",
    "f1_test_pm_foret  = []\n",
    "for max_prof in MAX_PROF_lst:\n",
    "    classifieur = RandomForestClassifier(n_estimators=40, max_depth=max_prof)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_pm_foret.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_pm_foret.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MAX_PROF_lst, f1_train_pm_foret, color='blue'  , label='Convergence    (Foret)')\n",
    "plt.plot(MAX_PROF_lst, f1_test_pm_foret , color='red'   , label='Generalisation (Foret)')\n",
    "plt.plot(MAX_PROF_lst, f1_train_pm      , color='green' , label='Convergence    (Arbre)')\n",
    "plt.plot(MAX_PROF_lst, f1_test_pm       , color='orange', label='Generalisation (Arbre)')\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Profondeur max de l\\'arbre')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Comparer la convergence des forêts et des arbres en terme de la profondeur maximale\n",
    "- Comparer la généralisation des forêts et des arbres en terme de la profondeur maximale\n",
    "- Justifier ces résultats (en indiquant pourquoi nous avons des oscillations dans F1 test des forêts)\n",
    "\n",
    "**Réponse**\n",
    "- la convergence de l'entrainement des forets est plus vite que celle des arbres\n",
    "- la convergence du teste des forets est plus importante que celle des arbres\n",
    "- la convergence des forets est plus vite que celle des arbres, car une forets utilise plusieurs sous arbres sur des sub-datasets différents et construit des arbres différents de petites profondeurs, leurs réunions représente l'ensemble d'apprentissage le mieux et donne une prédiction meilleur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.3. Observations minimales dans les feuilles\n",
    "\n",
    "Pour chaque nombre minimale des observations dans les feuilles, nous entraînons un forêt et nous mesurons sa convergence et sa performance en terme du score F1. \n",
    "Nous comparons les résultats avec les arbres équivalents (avec le même nombre des observations minimales)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train_mf_foret = []\n",
    "f1_test_mf_foret  = []\n",
    "for min_feuille in MIN_FEUILLE_lst:\n",
    "    classifieur = RandomForestClassifier(n_estimators=40, min_samples_leaf=min_feuille)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_mf_foret.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_mf_foret.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(MIN_FEUILLE_lst, f1_train_mf_foret, color='blue'  , label='Convergence    (Foret)')\n",
    "plt.plot(MIN_FEUILLE_lst, f1_test_mf_foret , color='red'   , label='Generalisation (Foret)')\n",
    "plt.plot(MIN_FEUILLE_lst, f1_train_mf      , color='green' , label='Convergence    (Arbre)')\n",
    "plt.plot(MIN_FEUILLE_lst, f1_test_mf       , color='orange', label='Generalisation (Arbre)')\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Observations minimales dans les feuilles')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Comparer la performance (convergence et généralisation) des arbres et des forêts en terme du nombre des observations dans les feuilles\n",
    "- Justifier \n",
    "\n",
    "**Réponse**\n",
    "- La performance diminue quand le nombre d'observations minimales dans les feuilles devienne important (pour la convergence et la généralisation des arbres et des forêts )\n",
    "- Comme Random Forest est une collection d'arbres de décision alors un sous-apprentissage est provoqué si le nombre d'observations minimales est tres **grand** (i.e prendre en compte de toutes les observations à un seul nœud), et un sur-apprentissage si il est tres **petit** (i.e prendre en compte d'au moins un échantillon à chaque nœud ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### II.2.4. Taille d'un Bootstrap\n",
    "\n",
    "Ici, on définit un pourcentage de la taille des Bootstrap par rapport la taille initiale du dataset. Pour chaque pourcentage, on entraîne un forêt et on test sa performance (convergence et généralisation) en utilisant la mesure F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POUR_OBS_lst = np.arange(0.1, 1, 0.01)\n",
    "\n",
    "f1_train_ech = []\n",
    "f1_test_ech  = []\n",
    "for pour_obs in POUR_OBS_lst:\n",
    "    classifieur = RandomForestClassifier(n_estimators=40, max_samples=pour_obs)\n",
    "    classifieur.fit(X_cars_train, Y_cars_train)\n",
    "    f1_train_ech.append(f1_score(Y_cars_train, classifieur.predict(X_cars_train), average='micro'))\n",
    "    f1_test_ech.append (f1_score(Y_cars_test , classifieur.predict(X_cars_test) , average='micro'))\n",
    "\n",
    "plt.plot(POUR_OBS_lst, f1_train_ech, color='blue', label='Convergence    (Foret)')\n",
    "plt.plot(POUR_OBS_lst, f1_test_ech , color='red' , label='Generalisation (Foret)')\n",
    "plt.ylabel('F1')\n",
    "plt.xlabel('Pourcentage du bootstap par rapport le dataset originale')\n",
    "plt.legend()\n",
    "#plt.rcParams[\"figure.figsize\"] = (3,7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO : Analyser les résultats**\n",
    "- Que remarquez-vous ?\n",
    "- Quelle est la raison pour laquelle la performance n'augmente pas d'une manière lisse ?\n",
    "\n",
    "**Réponse**\n",
    "- l'augmentation du pourcentage du bootstap par rapport le dataset originale améliore le f1 score.\n",
    "- les oscillations reviennent a l'aleatoire dans le choix des sub-datasets (chaque arbre de RF est entrainer sur un sous-ensemble des observations ). Plusieurs arbres sont entrainé sur différents sub-datasets, et plus tard, les résultats de tous les arbres sont agrégés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
